func0000000000000000:                   // @func0000000000000000
// %bb.0:                               // %entry
	mov	w8, #23                         // =0x17
	madd	x9, x2, x8, x1
	madd	x0, x9, x8, x0
	ret
                                        // -- End function
func00000000000000ff:                   // @func00000000000000ff
// %bb.0:                               // %entry
	add	x9, x2, x2, lsl #2
	mov	w8, #18                         // =0x12
	add	x9, x9, x1
	madd	x0, x9, x8, x0
	ret
                                        // -- End function
func0000000000000055:                   // @func0000000000000055
// %bb.0:                               // %entry
	mov	x8, #-24                        // =0xffffffffffffffe8
	mov	x9, #-60                        // =0xffffffffffffffc4
	madd	x8, x2, x8, x1
	madd	x0, x8, x9, x0
	ret
                                        // -- End function
func0000000000000044:                   // @func0000000000000044
// %bb.0:                               // %entry
	mov	x8, #-400                       // =0xfffffffffffffe70
	mov	w9, #365                        // =0x16d
	madd	x8, x2, x8, x1
	madd	x0, x8, x9, x0
	ret
                                        // -- End function
func0000000000000054:                   // @func0000000000000054
// %bb.0:                               // %entry
	mov	x8, #-60                        // =0xffffffffffffffc4
	madd	x9, x2, x8, x1
	madd	x0, x9, x8, x0
	ret
                                        // -- End function
func00000000000000fd:                   // @func00000000000000fd
// %bb.0:                               // %entry
	mov	w8, #60                         // =0x3c
	mov	w9, #16960                      // =0x4240
	madd	x8, x2, x8, x1
	movk	w9, #15, lsl #16
	madd	x0, x8, x9, x0
	ret
                                        // -- End function
func0000000000000040:                   // @func0000000000000040
// %bb.0:                               // %entry
	mov	x8, #-20864                     // =0xffffffffffffae80
	mov	x9, #4096                       // =0x1000
	movk	x8, #65534, lsl #16
	movk	x9, #54437, lsl #16
	madd	x8, x2, x8, x1
	movk	x9, #232, lsl #32
	madd	x0, x8, x9, x0
	ret
                                        // -- End function
func0000000000000050:                   // @func0000000000000050
// %bb.0:                               // %entry
	mov	w8, #60                         // =0x3c
	mov	w9, #16960                      // =0x4240
	madd	x8, x2, x8, x1
	movk	w9, #15, lsl #16
	madd	x0, x8, x9, x0
	ret
                                        // -- End function
func00000000000000f0:                   // @func00000000000000f0
// %bb.0:                               // %entry
	mov	w8, #59797                      // =0xe995
	movk	w8, #23505, lsl #16
	madd	x9, x2, x8, x1
	madd	x0, x9, x8, x0
	ret
                                        // -- End function
func00000000000000a0:                   // @func00000000000000a0
// %bb.0:                               // %entry
	mov	w8, #25354                      // =0x630a
	mov	w9, #25354                      // =0x630a
	movk	w8, #63551, lsl #16
	madd	x8, x2, x8, x1
	madd	x0, x8, x9, x0
	ret
                                        // -- End function
func0000000000000008:                   // @func0000000000000008
// %bb.0:                               // %entry
	mov	w8, #10                         // =0xa
	madd	x9, x2, x8, x1
	madd	x0, x9, x8, x0
	ret
                                        // -- End function
func00000000000000fa:                   // @func00000000000000fa
// %bb.0:                               // %entry
	mov	w8, #10                         // =0xa
	madd	x9, x2, x8, x1
	madd	x0, x9, x8, x0
	ret
                                        // -- End function
func00000000000000a8:                   // @func00000000000000a8
// %bb.0:                               // %entry
	mov	w8, #10                         // =0xa
	madd	x9, x2, x8, x1
	madd	x0, x9, x8, x0
	ret
                                        // -- End function
