func0000000000000026:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, lt
	ret

func0000000000000006:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, lt
	ret

func0000000000000004:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, lo
	ret

func0000000000000078:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, hi
	ret

func0000000000000061:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, eq
	ret

func0000000000000064:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, lo
	ret

func0000000000000044:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, lo
	ret

func0000000000000068:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, hi
	ret

func0000000000000001:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, eq
	ret

func0000000000000048:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, hi
	ret

func0000000000000041:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, eq
	ret

func000000000000004c:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, ne
	ret

func000000000000002a:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, gt
	ret

func0000000000000008:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, hi
	ret

func0000000000000074:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, lo
	ret

func000000000000000c:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, ne
	ret

func0000000000000021:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, eq
	ret

func000000000000002b:
	and	x8, x2, #0x1
	add	x8, x1, x8
	cmp	x0, x8
	cset	w0, ge
	ret

