func0000000000000075:                   // @func0000000000000075
// %bb.0:                               // %entry
	fmov	x9, d3
	fmov	x11, d2
	mov	w8, #5585                       // =0x15d1
	movk	w8, #2, lsl #16
	mov	x10, v3.d[1]
	mov	x12, v2.d[1]
	movi	v2.4s, #31, msl #16
	mul	x9, x9, x8
	mul	x11, x11, x8
	and	v2.16b, v4.16b, v2.16b
	mul	x10, x10, x8
	fmov	d3, x9
	mul	x8, x12, x8
	uaddw2	v1.2d, v1.2d, v2.4s
	uaddw	v0.2d, v0.2d, v2.2s
	fmov	d4, x11
	mov	v3.d[1], x10
	mov	v4.d[1], x8
	add	v1.2d, v1.2d, v3.2d
	add	v0.2d, v0.2d, v4.2d
	ret
                                        // -- End function
func000000000000007f:                   // @func000000000000007f
// %bb.0:                               // %entry
	fmov	x9, d3
	fmov	x11, d2
	mov	w8, #11544                      // =0x2d18
	movk	w8, #7, lsl #16
	mov	x10, v3.d[1]
	mov	x12, v2.d[1]
	movi	v2.4s, #31, msl #16
	mul	x9, x9, x8
	mul	x11, x11, x8
	and	v2.16b, v4.16b, v2.16b
	mul	x10, x10, x8
	fmov	d3, x9
	mul	x8, x12, x8
	uaddw2	v1.2d, v1.2d, v2.4s
	uaddw	v0.2d, v0.2d, v2.2s
	fmov	d4, x11
	mov	v3.d[1], x10
	mov	v4.d[1], x8
	add	v1.2d, v1.2d, v3.2d
	add	v0.2d, v0.2d, v4.2d
	ret
                                        // -- End function
func000000000000005d:                   // @func000000000000005d
// %bb.0:                               // %entry
	fmov	x9, d3
	fmov	x11, d2
	mov	x8, #-14765                     // =0xffffffffffffc653
	movk	x8, #65520, lsl #16
	mov	x10, v3.d[1]
	mov	x12, v2.d[1]
	movi	v2.4s, #31, msl #16
	mul	x9, x9, x8
	mul	x11, x11, x8
	and	v2.16b, v4.16b, v2.16b
	mul	x10, x10, x8
	fmov	d3, x9
	mul	x8, x12, x8
	uaddw2	v1.2d, v1.2d, v2.4s
	uaddw	v0.2d, v0.2d, v2.2s
	fmov	d4, x11
	mov	v3.d[1], x10
	mov	v4.d[1], x8
	add	v1.2d, v1.2d, v3.2d
	add	v0.2d, v0.2d, v4.2d
	ret
                                        // -- End function
func0000000000000055:                   // @func0000000000000055
// %bb.0:                               // %entry
	fmov	x9, d3
	fmov	x11, d2
	mov	w8, #11544                      // =0x2d18
	movk	w8, #7, lsl #16
	mov	x10, v3.d[1]
	mov	x12, v2.d[1]
	movi	v2.4s, #31, msl #16
	mul	x9, x9, x8
	mul	x11, x11, x8
	and	v2.16b, v4.16b, v2.16b
	mul	x10, x10, x8
	fmov	d3, x9
	mul	x8, x12, x8
	uaddw2	v1.2d, v1.2d, v2.4s
	uaddw	v0.2d, v0.2d, v2.2s
	fmov	d4, x11
	mov	v3.d[1], x10
	mov	v4.d[1], x8
	add	v1.2d, v1.2d, v3.2d
	add	v0.2d, v0.2d, v4.2d
	ret
                                        // -- End function
func000000000000005c:                   // @func000000000000005c
// %bb.0:                               // %entry
	fmov	x9, d3
	fmov	x11, d2
	mov	w8, #11283                      // =0x2c13
	movk	w8, #10, lsl #16
	mov	x10, v3.d[1]
	mov	x12, v2.d[1]
	movi	v2.4s, #31, msl #16
	mul	x9, x9, x8
	mul	x11, x11, x8
	and	v2.16b, v4.16b, v2.16b
	mul	x10, x10, x8
	fmov	d3, x9
	mul	x8, x12, x8
	uaddw2	v1.2d, v1.2d, v2.4s
	uaddw	v0.2d, v0.2d, v2.2s
	fmov	d4, x11
	mov	v3.d[1], x10
	mov	v4.d[1], x8
	add	v1.2d, v1.2d, v3.2d
	add	v0.2d, v0.2d, v4.2d
	ret
                                        // -- End function
func000000000000004c:                   // @func000000000000004c
// %bb.0:                               // %entry
	fmov	x9, d3
	fmov	x11, d2
	mov	w8, #11283                      // =0x2c13
	movk	w8, #10, lsl #16
	mov	x10, v3.d[1]
	mov	x12, v2.d[1]
	movi	v2.4s, #31, msl #16
	mul	x9, x9, x8
	mul	x11, x11, x8
	and	v2.16b, v4.16b, v2.16b
	mul	x10, x10, x8
	fmov	d3, x9
	mul	x8, x12, x8
	uaddw2	v1.2d, v1.2d, v2.4s
	uaddw	v0.2d, v0.2d, v2.2s
	fmov	d4, x11
	mov	v3.d[1], x10
	mov	v4.d[1], x8
	add	v1.2d, v1.2d, v3.2d
	add	v0.2d, v0.2d, v4.2d
	ret
                                        // -- End function
func0000000000000040:                   // @func0000000000000040
// %bb.0:                               // %entry
	fmov	x9, d3
	fmov	x11, d2
	mov	w8, #11544                      // =0x2d18
	movk	w8, #7, lsl #16
	mov	x10, v3.d[1]
	mov	x12, v2.d[1]
	movi	v2.4s, #31, msl #16
	mul	x9, x9, x8
	mul	x11, x11, x8
	and	v2.16b, v4.16b, v2.16b
	mul	x10, x10, x8
	fmov	d3, x9
	mul	x8, x12, x8
	uaddw2	v1.2d, v1.2d, v2.4s
	uaddw	v0.2d, v0.2d, v2.2s
	fmov	d4, x11
	mov	v3.d[1], x10
	mov	v4.d[1], x8
	add	v1.2d, v1.2d, v3.2d
	add	v0.2d, v0.2d, v4.2d
	ret
                                        // -- End function
