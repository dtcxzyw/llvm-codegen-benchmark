func0000000000000075:                   // @func0000000000000075
// %bb.0:                               // %entry
	mov	x8, v2.d[1]
	mov	x11, v3.d[1]
	mov	x9, #-14765                     // =0xffffffffffffc653
	fmov	x10, d2
	fmov	x13, d3
	movk	x9, #65520, lsl #16
	fmov	x14, d1
	fmov	x16, d0
	mov	w17, #2097151                   // =0x1fffff
	mov	x12, v1.d[1]
	mov	x15, v0.d[1]
	dup	v0.2d, x17
	mul	x10, x10, x9
	mul	x8, x8, x9
	and	v3.16b, v4.16b, v0.16b
	and	v0.16b, v5.16b, v0.16b
	mul	x11, x11, x9
	fmov	d1, x10
	mul	x9, x13, x9
	mov	w13, #64359                     // =0xfb67
	movk	w13, #9, lsl #16
	mul	x14, x14, x13
	mov	v1.d[1], x8
	mul	x16, x16, x13
	fmov	d2, x9
	mul	x12, x12, x13
	fmov	d4, x14
	mul	x13, x15, x13
	mov	v2.d[1], x11
	fmov	d5, x16
	mov	v4.d[1], x12
	add	v2.2d, v2.2d, v0.2d
	add	v0.2d, v1.2d, v3.2d
	mov	v5.d[1], x13
	add	v1.2d, v2.2d, v4.2d
	add	v0.2d, v0.2d, v5.2d
	ret
                                        // -- End function
func0000000000000055:                   // @func0000000000000055
// %bb.0:                               // %entry
	mov	x8, v2.d[1]
	mov	x11, v3.d[1]
	mov	w9, #11283                      // =0x2c13
	fmov	x10, d2
	fmov	x13, d3
	movk	w9, #10, lsl #16
	fmov	x14, d1
	fmov	x16, d0
	mov	w17, #2097151                   // =0x1fffff
	mov	x12, v1.d[1]
	mov	x15, v0.d[1]
	dup	v0.2d, x17
	mul	x10, x10, x9
	mul	x8, x8, x9
	and	v3.16b, v4.16b, v0.16b
	and	v0.16b, v5.16b, v0.16b
	mul	x11, x11, x9
	fmov	d1, x10
	mul	x9, x13, x9
	mov	w13, #11544                     // =0x2d18
	movk	w13, #7, lsl #16
	mul	x14, x14, x13
	mov	v1.d[1], x8
	mul	x16, x16, x13
	fmov	d2, x9
	mul	x12, x12, x13
	fmov	d4, x14
	mul	x13, x15, x13
	mov	v2.d[1], x11
	fmov	d5, x16
	mov	v4.d[1], x12
	add	v2.2d, v2.2d, v0.2d
	add	v0.2d, v1.2d, v3.2d
	mov	v5.d[1], x13
	add	v1.2d, v2.2d, v4.2d
	add	v0.2d, v0.2d, v5.2d
	ret
                                        // -- End function
func0000000000000044:                   // @func0000000000000044
// %bb.0:                               // %entry
	mov	x8, v2.d[1]
	mov	x11, v3.d[1]
	mov	w9, #11283                      // =0x2c13
	fmov	x10, d2
	fmov	x13, d3
	movk	w9, #10, lsl #16
	fmov	x14, d1
	fmov	x16, d0
	mov	w17, #2097151                   // =0x1fffff
	mov	x12, v1.d[1]
	mov	x15, v0.d[1]
	dup	v0.2d, x17
	mul	x10, x10, x9
	mul	x8, x8, x9
	and	v3.16b, v4.16b, v0.16b
	and	v0.16b, v5.16b, v0.16b
	mul	x11, x11, x9
	fmov	d1, x10
	mul	x9, x13, x9
	mov	w13, #11544                     // =0x2d18
	movk	w13, #7, lsl #16
	mul	x14, x14, x13
	mov	v1.d[1], x8
	mul	x16, x16, x13
	fmov	d2, x9
	mul	x12, x12, x13
	fmov	d4, x14
	mul	x13, x15, x13
	mov	v2.d[1], x11
	fmov	d5, x16
	mov	v4.d[1], x12
	add	v2.2d, v2.2d, v0.2d
	add	v0.2d, v1.2d, v3.2d
	mov	v5.d[1], x13
	add	v1.2d, v2.2d, v4.2d
	add	v0.2d, v0.2d, v5.2d
	ret
                                        // -- End function
func0000000000000000:                   // @func0000000000000000
// %bb.0:                               // %entry
	mov	x8, v2.d[1]
	mov	x11, v3.d[1]
	mov	w9, #11283                      // =0x2c13
	fmov	x10, d2
	fmov	x13, d3
	movk	w9, #10, lsl #16
	fmov	x14, d1
	fmov	x16, d0
	mov	w17, #2097151                   // =0x1fffff
	mov	x12, v1.d[1]
	mov	x15, v0.d[1]
	dup	v0.2d, x17
	mul	x10, x10, x9
	mul	x8, x8, x9
	and	v3.16b, v4.16b, v0.16b
	and	v0.16b, v5.16b, v0.16b
	mul	x11, x11, x9
	fmov	d1, x10
	mul	x9, x13, x9
	mov	w13, #11544                     // =0x2d18
	movk	w13, #7, lsl #16
	mul	x14, x14, x13
	mov	v1.d[1], x8
	mul	x16, x16, x13
	fmov	d2, x9
	mul	x12, x12, x13
	fmov	d4, x14
	mul	x13, x15, x13
	mov	v2.d[1], x11
	fmov	d5, x16
	mov	v4.d[1], x12
	add	v2.2d, v2.2d, v0.2d
	add	v0.2d, v1.2d, v3.2d
	mov	v5.d[1], x13
	add	v1.2d, v2.2d, v4.2d
	add	v0.2d, v0.2d, v5.2d
	ret
                                        // -- End function
